Machine Learning Course Project: Human Activity Recognition
----------------------------------------------

<br>

S. Massie <br>
Coursera / Johns Hopkins University: Practical Machine Learning <br>
October 25, 2015 <br.

Note: this R markdown file requires the following packages: 
- dplyr
- caret

<br>

## 1. Overview

The dataset <http://groupware.les.inf.puc-rio.br/har> includes electronic sensor data from six test subjects who performed a barbell exercise.  Each record was then assigned a Class from "A" to "E" based on whether the exercise was performed correctly.

In this project, I build a model to predict the Class from the exercise data.  Data was split into "training" (80%) and "probe" (20%) datasets.  100 variables of primarily blank or NA variables were removed, and some observations which appeared to be from a different dataset were filtered out.  Additionally,  quantative (non-factor) variables were combined using Principal Components Analysis (PCA), and outliers representing 0.67% of the dataset were removed.

Using a Random Forest method, the model was able to achieve 100% accuracy in-sample and 99.95% accuracy in the "probe" sample.  

<br>

## Getting and cleaning data

Data was downloaded from the Coursera website and extracted to the "exTrain" data frame.

```{r, echo=FALSE, results = 'hide', cache=TRUE}
## Preparing workspace and loading data
setwd("~/Desktop/Coursera/8 Machine Learning")

urlTrain <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
urlTest <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

if(!file.exists("pml-training.csv")){
    download.file(urlTrain, "pml-training.csv", method = "curl")
}

if(!file.exists("pml-testing.csv")){
    download.file(urlTest, "pml-testing.csv", method = "curl")
}

if(!exists("exTrain")) exTrain <- read.csv("pml-training.csv")
if(!exists("exTest")) exTest <- read.csv("pml-testing.csv")
```

Notice that a large number of variables (100) are mostly NA or blank values, while the rest (60) have none.  Remove these variables.

```{r, results='hide', cache=TRUE}
# Replace blank values with NAs
exTrain <- replace(exTrain, exTrain=="", NA)

# Create list of columns which contain NAs:
NAcols <- logical(length = 160)
for(i in 1:160) NAcols[i] <- any(is.na(exTrain[,i]))

# Select only columns which do not contain NAs
library(dplyr)

exTrain <- select(exTrain, which(!NAcols))
```

<br>

## Data slicing

Subdivide the original training set into "training" (80%) and "probe" (20%) sets so that we can perform cross validation.

```{r, echo=FALSE, results='hide', cache=TRUE}
library(caret)

set.seed(111)
trainrows <- createDataPartition(exTrain$classe, p = 0.8, list = FALSE)

training <- exTrain[trainrows,]
probe <- exTrain[-trainrows,]
```

<br>

## Feature selection

Check for near-zero-covariate variables:
```{r, echo=FALSE, results = 'hide', cache=TRUE}
nzv <- nearZeroVar(training, saveMetrics = TRUE)
subset(nzv, nzv = TRUE)
```

Note that "new window" is an NZV variable.  On exploration, records with "new window" = "yes" also turn out to be the same records for the excluded NA columns were mostly complete.  This suggests that these observations were collected differently, so they are excluded.

Also remove timestamp variables for simplicity.  I assume that the timestamp variable by itself, would not very useful as a predictor of *future* lifting, even if it proves useful in cross validation.

```{r, results='hide', cache=TRUE}
training <- filter(training, new_window == "no")
training <- select(training, -new_window, -raw_timestamp_part_1, 
                   -raw_timestamp_part_2, -cvtd_timestamp)
```

<br>

## Pre-processing

Note that many variables in this dataset appear to be the same measurements of x, y, and z axis.  For instance, "accel_arm_x", "accel_arm_y", and "accel_arm_z" are three variables.

Use PCA to create combinations of quantitative motion variables, and recombine the PCA variables with factor variables:

<br>

```{r, results='hide', cache=TRUE}
# Select only quantiative variables
training_quant <- select(training, -X, -user_name, -num_window, -classe)

# Perform PCA on these variables to capture 95% of variance.  (Can try other tresholds later)
PCAvars <- preProcess(training_quant, method = "pca", thresh = 0.95)

# Predict PCA values for training dataset, and recombine with factor variables
training_PCA <- predict(PCAvars, training_quant)

training_PCA <- mutate(training_PCA, user_name=training$user_name,
                num_window = training$num_window, classe = training$classe)
```

<br>

## Checking for outliers

After creating exploratory plots of "classe" aginst factor variables, "X", "user_name", and "num_window", notice there is exact correlation between "classe" and "X", so we also remove X from the dataset.

Create an exploratory plot showing histograms of PCA variables:

```{r, cache=TRUE}
par(mfrow = c(5,5), mar = c(2,2,1,0.5))
for(i in 1:25){
    hist(training_PCA[,i])
}
```

Notice that variables tend to be normally distributed after PCA transformation.  However, some variables, especially PC3 and PC4, suffer from extreme outliers.  

Flag rows with outliers:
```{r, results='hide',cache=TRUE}
# Remove PCA variables which are more than 6 units (standard deviations) from mean
outly <- function(x, range) which(abs(x) >= range)

outliers <- numeric()
for(i in 1:25){
    outliers <- unique(c(outliers,outly(training_PCA[,i], 6)))
}
```

These values represent only 0.67% of the dataset.  Since we are interested predicting in "correct" behaviors, as opposed to rare events, we will remove these outliers to improve model accuracy while training the model.

<br>

## Model training

I choose a classification tree-based model over a GLM-based model because we are trying to model a categorial variable.  Here, random forest is used.

```{r, results='hide', cache=TRUE}
# Training: caret package version
if(!exists("model2")){
    model2 <- train(classe ~., data=training_PCA[-outliers,], method = "rf")
}
```

<br>

## Error rates and Cross validation

First, check in-sample error rate.
```{r, echo=FALSE, results='hide', cache=TRUE}
train_y <- predict(model2, newdata = select(training_PCA, -classe))

confusionMatrix(train_y, training_PCA$classe)
```

Predictions appear to be 100% accurate on the in-sample data.  But we still need to estimate the error rate for out-of-sample data, i.e., the "probe" dataset.

Preprocess "probe" dataset according to same steps as "training" dataset:
```{r, echo=FALSE, results='hide', cache=TRUE}
probe <- filter(probe, new_window == "no")    

probe <- select(probe, -new_window, -raw_timestamp_part_1, 
        -raw_timestamp_part_2, -cvtd_timestamp)
probe_quant <- select(probe, -X, -user_name, -num_window, -classe)

probe_PCA <- predict(PCAvars, probe_quant)

probe_PCA <- mutate(probe_PCA, X=probe$X, user_name=probe$user_name,
                num_window = probe$num_window, classe = probe$classe)
probe_PCA <- select(probe_PCA, -X)
```

Next, predict values in probe dataset using model.
```{r, results='hide',cache=TRUE}
probe_y <- predict(model2, newdata = select(probe_PCA, -classe))
```

Compare predicted to actual values and show confusion matrix:
```{r}
confM <- confusionMatrix(probe_y, probe_PCA$classe)
confM
```

The model was 99.95% accurate (2 wrong predictions out of 3853).  So the model is accurate enough to predict from the test dataset.

<br>

## Predicting test values

As a final step, the model is used to predict values for the "test" dataset of 20 records provided by coursera.


```{r, echo=FALSE, results='hide',cache=TRUE}
exTest <- select(exTest, which(!NAcols))
test_quant <- select(exTest, -new_window, -raw_timestamp_part_1, 
                     -raw_timestamp_part_2, -cvtd_timestamp, -X, -user_name,
                     -num_window, -problem_id)
test_PCA <- predict(PCAvars, test_quant)
test_PCA <- mutate(test_PCA, user_name=exTest$user_name,
                num_window = exTest$num_window, problem_id = exTest$problem_id)
```

Predict new values.
```{r, cache=TRUE}
test_y <- predict(model2, newdata = test_PCA)
test_y
```

The model generates a set of predictions for the test variable.

<br>

## Old code / appendix

Counting NA values:

```{r, echo=FALSE, results='hide'}
# Counting NA values
# NAcount <- function(x) sum((is.na(x)|(x==""))*1)
```

Plotting Y aginst factor Xs.
```{r, echo=FALSE, results='hide'}
# with(training_PCA,(plot(X,classe))
# with(training_PCA,(plot(num_window,classe))
# with(training_PCA,(plot(user_name,classe))

# featurePlot(x=training_PCA[,1:27], y = training_PCA$classe, plot = "pairs")

```

Checking ranges.
```{r, echo=FALSE, results='hide'}
# Xmax <- sapply(training_PCA[,1:25], FUN = max)
# Xmin <- sapply(training_PCA[,1:25], FUN = min)
# Xrange <- cbind(Xmin, Xmax)
```

Creating model with RandomForest package:
```{r, echo=FALSE, results='hide', cache=TRUE}
# Training: randomForest package version

# library(randomForest)
# 
# if(!exists("model1")){
#     model1 <- randomForest(classe ~ ., data=training_PCA[-outliers,])
# }
```
